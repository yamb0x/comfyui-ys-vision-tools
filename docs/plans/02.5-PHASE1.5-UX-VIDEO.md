# Phase 1.5: UX Polish & Video Support

> **üéØ Phase 1.5 Goal:** Polish user experience, add dynamic sizing, and enable proper video workflows before expanding to Phase 2.

**Status:** Planning
**Prerequisites:** ‚úÖ Phase 1 Complete & Deployed
**Focus:** User experience, video support, workflow simplification

---

## üéØ Phase 1.5 Objectives

### Problems to Solve

1. **Manual Size Entry is Error-Prone**
   - Users must manually match image dimensions
   - Dimension mismatches cause rendering errors
   - No dynamic adaptation to input size

2. **Video Motion Detection Not Working**
   - Optical flow needs previous frame
   - Current workflow has no frame offset capability
   - Cannot test motion-based detection properly

3. **Overwhelming Options**
   - Line Link Renderer has 12+ curves and 9+ styles
   - Users don't know where to start
   - Need quick preset configurations

4. **Glow Effects Too Strong by Default**
   - Dot renderer glow always on
   - Users want subtle effects first

5. **Color Distribution Not Smart**
   - Palette Map outputs single palette
   - No way to distribute N colors to multiple nodes
   - Manual color picking for each node

---

## üì¶ New Nodes (2 additions)

### 1. Image/Video Size Detector Node üìê

**Purpose:** Automatically detect and output image/video dimensions for dynamic workflows

**Node Name:** `YS_ImageSizeDetector`
**Display Name:** "Image Size Detector üìê"
**Category:** "YS-vision-tools/Utilities"

#### Input Types
```python
@classmethod
def INPUT_TYPES(cls):
    return {
        "required": {
            "image": ("IMAGE",),  # ComfyUI IMAGE type (BHWC)
        }
    }
```

#### Return Types
```python
RETURN_TYPES = ("IMAGE", "INT", "INT", "STRING")
RETURN_NAMES = ("image", "width", "height", "dimensions")
```

#### Outputs
- **image** (IMAGE): Pass-through of input image
- **width** (INT): Image width in pixels
- **height** (INT): Image height in pixels
- **dimensions** (STRING): Formatted string "WxH" (e.g., "1920x1080")

#### Implementation
```python
def execute(self, image):
    """Extract dimensions from ComfyUI image tensor"""
    # image shape: (B, H, W, C) - BHWC format
    batch, height, width, channels = image.shape

    dimensions_str = f"{width}x{height}"

    return (
        image,          # Pass-through
        width,          # INT for node inputs
        height,         # INT for node inputs
        dimensions_str  # STRING for display
    )
```

#### Workflow Usage
```
Load Image
    ‚Üì
Image Size Detector üìê
    ‚Üì (image pass-through)
    ‚îú‚îÄ‚Üí Track Detect
    ‚Üì (width, height as INTs)
    ‚îú‚îÄ‚Üí Line Link Renderer (image_width, image_height)
    ‚îú‚îÄ‚Üí Dot Renderer (image_width, image_height)
    ‚îî‚îÄ‚Üí Any future nodes needing dimensions
```

#### Benefits
- ‚úÖ No manual size entry needed
- ‚úÖ Automatic dimension matching
- ‚úÖ Works with any image/video size
- ‚úÖ Eliminates dimension mismatch errors
- ‚úÖ Clean pass-through design

---

### 2. Video Frame Offset Node üé¨

**Purpose:** Provide frame offset capability for video workflows, enabling motion detection and temporal effects

**Node Name:** `YS_VideoFrameOffset`
**Display Name:** "Video Frame Offset üé¨"
**Category:** "YS-vision-tools/Video"

#### Input Types
```python
@classmethod
def INPUT_TYPES(cls):
    return {
        "required": {
            "images": ("IMAGE",),  # Batch of frames from video
            "offset": ("INT", {
                "default": -1,
                "min": -10,
                "max": 0,
                "step": 1,
                "display": "slider"
            }),
            "mode": (["hold_first", "repeat_edge", "black_frame"],),
        }
    }
```

#### Return Types
```python
RETURN_TYPES = ("IMAGE", "IMAGE", "MASK")
RETURN_NAMES = ("current_frame", "offset_frame", "valid_mask")
```

#### Outputs
- **current_frame** (IMAGE): Current frame (index N)
- **offset_frame** (IMAGE): Previous frame (index N + offset)
- **valid_mask** (MASK): Indicates if offset frame is valid (for first frame handling)

#### Implementation
```python
def execute(self, images, offset=-1, mode="hold_first"):
    """
    Extract offset frame from video batch

    Args:
        images: (B, H, W, C) batch of frames
        offset: Frame offset (-1 = previous frame, -2 = 2 frames back)
        mode: How to handle edge cases (first frame)
    """
    batch_size = images.shape[0]

    current_frames = []
    offset_frames = []
    valid_masks = []

    for i in range(batch_size):
        current_frame = images[i:i+1]  # Keep batch dim

        offset_idx = i + offset

        if offset_idx < 0:
            # Handle first frames based on mode
            if mode == "hold_first":
                offset_frame = images[0:1]  # Use first frame
            elif mode == "repeat_edge":
                offset_frame = current_frame  # Use current frame
            else:  # black_frame
                offset_frame = torch.zeros_like(current_frame)
            valid = False
        else:
            offset_frame = images[offset_idx:offset_idx+1]
            valid = True

        current_frames.append(current_frame)
        offset_frames.append(offset_frame)
        valid_masks.append(torch.tensor([[valid]], dtype=torch.float32))

    return (
        torch.cat(current_frames, dim=0),
        torch.cat(offset_frames, dim=0),
        torch.cat(valid_masks, dim=0)
    )
```

#### Workflow Usage for Motion Detection
```
Load Video (VideoHelperSuite)
    ‚Üì
Video Frame Offset üé¨ (offset=-1)
    ‚Üì (current_frame, offset_frame)
    ‚îú‚îÄ‚Üí Track Detect
    ‚îÇ     ‚îú‚îÄ image: current_frame
    ‚îÇ     ‚îî‚îÄ prev_image: offset_frame (for optical_flow method)
    ‚Üì
Line Link Renderer (motion-based connections)
```

#### Edge Case Handling
- **offset=-1, frame 0**: No previous frame exists
  - `hold_first`: Use frame 0 as "previous"
  - `repeat_edge`: Use frame 0 as both current and previous
  - `black_frame`: Use black frame as previous
- **valid_mask**: Allows nodes to skip first frame if needed

#### Benefits
- ‚úÖ Enables proper optical flow detection
- ‚úÖ Supports motion-based tracking
- ‚úÖ Handles edge cases cleanly
- ‚úÖ Works with VideoHelperSuite
- ‚úÖ Flexible offset range (-10 to 0)

---

## üîß Node Updates (3 modifications)

### 3. Dot Renderer - Default Glow Off

**File:** `nodes/dot_renderer.py`

**Changes:**
```python
# BEFORE:
"glow_intensity": ("FLOAT", {"default": 0.5, "min": 0.0, "max": 1.0}),

# AFTER:
"glow_intensity": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 1.0}),
```

**Reason:** Glow effects should be opt-in, not default. Users want clean dots first.

---

### 4. Palette Map - Smart Color Distribution

**File:** `nodes/palette_map.py`

**New Feature:** Multi-color output for distributing to multiple nodes

#### Updated Return Types
```python
RETURN_TYPES = ("PALETTE", "COLOR", "COLOR", "COLOR", "COLOR", "COLOR")
RETURN_NAMES = ("palette", "color_1", "color_2", "color_3", "color_4", "color_5")
```

#### Implementation
```python
def execute(self, palette_type, num_colors, **kwargs):
    """Generate palette and extract N individual colors"""

    # Generate full palette (existing logic)
    palette = self._generate_palette(palette_type, num_colors)

    # Smart color distribution:
    # Extract 5 evenly-spaced colors from palette
    indices = np.linspace(0, num_colors - 1, 5, dtype=int)

    colors = []
    for idx in indices:
        color = palette[idx]  # (R, G, B) in [0, 1]
        colors.append(color)

    return (
        palette,      # Full palette array
        colors[0],    # Color 1 (start)
        colors[1],    # Color 2 (25%)
        colors[2],    # Color 3 (50%)
        colors[3],    # Color 4 (75%)
        colors[4],    # Color 5 (end)
    )
```

#### Workflow Usage
```
Palette Map üé® (rainbow, 256 colors)
    ‚Üì (5 individual colors extracted)
    ‚îú‚îÄ‚Üí color_1 ‚Üí Line Link Renderer
    ‚îú‚îÄ‚Üí color_2 ‚Üí Dot Renderer
    ‚îú‚îÄ‚Üí color_3 ‚Üí Line Link Renderer (second instance)
    ‚îú‚îÄ‚Üí color_4 ‚Üí Dot Renderer (second instance)
    ‚îî‚îÄ‚Üí color_5 ‚Üí Future nodes
```

#### Benefits
- ‚úÖ One palette feeds multiple nodes
- ‚úÖ Automatic color distribution
- ‚úÖ Evenly-spaced color picking
- ‚úÖ No manual color selection needed
- ‚úÖ Maintains color harmony

---

### 5. Line Link Renderer - Preset Configurations

**File:** `nodes/line_link_renderer.py`

**New Input:** Preset selection dropdown

#### Updated Input Types
```python
@classmethod
def INPUT_TYPES(cls):
    return {
        "required": {
            "preset": ([
                "custom",           # User configures manually
                "clean_technical",  # Straight lines, solid, minimal
                "organic_flow",     # Curves, gradient fade
                "electric_energy",  # Spirals, electric style
                "particle_swarm",   # Particle trails, pulsing
                "neural_network",   # Delaunay, wave style
                "quantum_field",    # Field lines, electric
                "minimal_dots",     # Straight, dotted, simple
                "cosmic_web",       # Voronoi, gradient fade
            ], {"default": "custom"}),

            # Existing inputs follow...
            "curve_type": (...),
            "line_style": (...),
            # ... etc
        }
    }
```

#### Preset Configurations

**1. clean_technical** - For analysis and debugging
```python
{
    "curve_type": "straight",
    "line_style": "solid",
    "graph_mode": "knn",
    "k_neighbors": 3,
    "line_width_px": 1.5,
    "color": [1.0, 1.0, 1.0],  # White
    "opacity": 0.8,
    "antialiasing": "2x"
}
```

**2. organic_flow** - Smooth, natural aesthetic
```python
{
    "curve_type": "catmull_rom",
    "line_style": "gradient_fade",
    "graph_mode": "knn",
    "k_neighbors": 4,
    "line_width_px": 2.5,
    "color": [0.3, 0.8, 0.9],  # Cyan
    "opacity": 0.7,
    "antialiasing": "4x"
}
```

**3. electric_energy** - High-energy lightning effect
```python
{
    "curve_type": "logarithmic_spiral",
    "line_style": "electric",
    "graph_mode": "knn",
    "k_neighbors": 5,
    "line_width_px": 3.0,
    "color": [0.5, 0.7, 1.0],  # Electric blue
    "opacity": 0.9,
    "antialiasing": "2x"
}
```

**4. particle_swarm** - Particle system aesthetic
```python
{
    "curve_type": "elastic",
    "line_style": "particle_trail",
    "graph_mode": "radius",
    "connection_radius_px": 150,
    "line_width_px": 1.0,
    "color": [1.0, 0.5, 0.2],  # Orange
    "opacity": 0.6,
    "antialiasing": "2x"
}
```

**5. neural_network** - AI/tech visualization
```python
{
    "curve_type": "straight",
    "line_style": "wave",
    "graph_mode": "delaunay",
    "line_width_px": 2.0,
    "color": [0.2, 1.0, 0.5],  # Green
    "opacity": 0.75,
    "antialiasing": "4x"
}
```

**6. quantum_field** - Field line simulation
```python
{
    "curve_type": "field_lines",
    "line_style": "electric",
    "graph_mode": "knn",
    "k_neighbors": 6,
    "line_width_px": 2.5,
    "color": [0.8, 0.3, 1.0],  # Purple
    "opacity": 0.8,
    "antialiasing": "2x"
}
```

**7. minimal_dots** - Clean, simple connections
```python
{
    "curve_type": "straight",
    "line_style": "dotted",
    "graph_mode": "knn",
    "k_neighbors": 2,
    "line_width_px": 1.5,
    "color": [0.9, 0.9, 0.9],  # Light gray
    "opacity": 0.5,
    "antialiasing": "2x"
}
```

**8. cosmic_web** - Voronoi-based organic structure
```python
{
    "curve_type": "cubic_bezier",
    "line_style": "gradient_fade",
    "graph_mode": "voronoi",
    "line_width_px": 2.0,
    "color": [0.6, 0.4, 0.9],  # Lavender
    "opacity": 0.7,
    "antialiasing": "4x"
}
```

#### Implementation
```python
def execute(self, preset, **kwargs):
    """Apply preset or use custom settings"""

    # If preset is "custom", use provided kwargs
    if preset == "custom":
        settings = kwargs
    else:
        # Load preset configuration
        settings = self.PRESETS[preset].copy()
        # Allow user overrides for specific params
        for key, value in kwargs.items():
            if value is not None:  # User can override preset
                settings[key] = value

    # Continue with rendering using settings
    return self._render_lines(settings)
```

#### Benefits
- ‚úÖ Quick starting points for users
- ‚úÖ Explore different aesthetics easily
- ‚úÖ Learn what settings create which effects
- ‚úÖ Still allows full customization
- ‚úÖ Presets can be overridden individually

---

## üìã Implementation Plan

### Week 1: Core Infrastructure (3-4 days)

**Day 1-2: Image Size Detector Node**
1. Create `nodes/image_size_detector.py`
2. Implement dimension extraction from BHWC tensors
3. Test with various image sizes
4. Add to node registration

**Day 3-4: Video Frame Offset Node**
1. Create `nodes/video_frame_offset.py`
2. Implement frame batching and offset logic
3. Test edge cases (first frame, invalid offsets)
4. Add to node registration

### Week 2: Enhancements (3-4 days)

**Day 5: Update Existing Nodes**
1. Change Dot Renderer glow default to 0.0
2. Test that glow still works when enabled

**Day 6-7: Palette Map Smart Distribution**
1. Update Palette Map return types
2. Implement smart N-color extraction
3. Test color distribution logic
4. Update documentation

**Day 8: Line Link Renderer Presets**
1. Define 8 preset configurations
2. Add preset input dropdown
3. Implement preset loading logic
4. Test all presets visually

### Week 3: Testing & Documentation (2-3 days)

**Day 9-10: User Testing Phase**
1. Create test workflows for each new feature
2. Test video workflows with VideoHelperSuite
3. Verify optical flow with frame offset
4. Test all presets visually
5. Get user feedback

**Day 11: Documentation**
1. Update README with new nodes
2. Create video workflow examples
3. Document preset options
4. Update TROUBLESHOOTING if needed

---

## üß™ Testing Plan - Phase 1.5

### Test Workflow 1: Dynamic Sizing
```
Load Image (any size: 512x512, 1920x1080, 4K, etc.)
    ‚Üì
Image Size Detector üìê
    ‚Üì (auto-detect dimensions)
Line Link Renderer
    ‚îú‚îÄ width: from size detector
    ‚îî‚îÄ height: from size detector
    ‚Üì
‚úÖ Should render correctly at any image size
```

**Success Criteria:**
- Works with 512x512, 1024x1024, 1920x1080, 3840x2160
- No manual dimension entry needed
- No dimension mismatch errors

---

### Test Workflow 2: Video Motion Detection
```
Load Video (VideoHelperSuite) - 30 fps, 3 seconds = 90 frames
    ‚Üì
Video Frame Offset üé¨ (offset=-1)
    ‚Üì (current + previous frames)
Track Detect
    ‚îú‚îÄ image: current_frame
    ‚îú‚îÄ prev_image: offset_frame
    ‚îî‚îÄ method: optical_flow
    ‚Üì
Line Link Renderer
    ‚Üì
‚úÖ Should track motion properly
```

**Success Criteria:**
- Optical flow detects moving objects
- First frame handled gracefully
- Motion trails visible in output
- No crashes or errors

---

### Test Workflow 3: Palette Distribution
```
Palette Map üé® (rainbow, 256 colors)
    ‚Üì (5 colors extracted)
    ‚îú‚îÄ‚Üí color_1 ‚Üí Line Link Renderer (instance 1)
    ‚îú‚îÄ‚Üí color_2 ‚Üí Dot Renderer (instance 1)
    ‚îú‚îÄ‚Üí color_3 ‚Üí Line Link Renderer (instance 2)
    ‚îú‚îÄ‚Üí color_4 ‚Üí Dot Renderer (instance 2)
    ‚îî‚îÄ‚Üí color_5 ‚Üí Composite Over (tint)
    ‚Üì
‚úÖ Each node gets harmonious color from palette
```

**Success Criteria:**
- 5 distinct colors output
- Colors evenly distributed across palette
- Colors maintain harmony (all from same palette)
- No manual color picking needed

---

### Test Workflow 4: Preset Exploration
```
Load Image
    ‚Üì
Track Detect
    ‚Üì
Test ALL 8 Presets:
    ‚îú‚îÄ clean_technical
    ‚îú‚îÄ organic_flow
    ‚îú‚îÄ electric_energy
    ‚îú‚îÄ particle_swarm
    ‚îú‚îÄ neural_network
    ‚îú‚îÄ quantum_field
    ‚îú‚îÄ minimal_dots
    ‚îî‚îÄ cosmic_web
    ‚Üì
‚úÖ Each preset creates distinct visual style
```

**Success Criteria:**
- All 8 presets render successfully
- Visual styles are clearly different
- Performance acceptable for all presets
- Presets can be overridden with custom values

---

### Test Workflow 5: Full Video Pipeline
```
Load Video (VideoHelperSuite)
    ‚Üì
Image Size Detector üìê
    ‚Üì (dimensions + pass-through)
Video Frame Offset üé¨ (offset=-1)
    ‚Üì
Track Detect (optical_flow with prev_image)
    ‚Üì
Line Link Renderer (preset: particle_swarm)
    ‚îú‚îÄ width: from size detector
    ‚îî‚îÄ height: from size detector
    ‚Üì
Palette Map üé®
    ‚Üì (color_1)
Line Link Renderer (apply color)
    ‚Üì
Composite Over
    ‚Üì
‚úÖ Complete video workflow with motion tracking
```

**Success Criteria:**
- Video processes all frames
- Motion detection works correctly
- Size auto-detected from video
- Preset applied successfully
- Color from palette distributed
- Output video has motion trails

---

## üìä Phase 1.5 Success Criteria

### Implementation Complete When:

**New Nodes:**
- ‚úÖ Image Size Detector implemented and tested
- ‚úÖ Video Frame Offset implemented and tested
- ‚úÖ Both nodes registered in ComfyUI
- ‚úÖ Both nodes visible in menu

**Node Updates:**
- ‚úÖ Dot Renderer glow default changed to 0.0
- ‚úÖ Palette Map smart distribution implemented
- ‚úÖ Line Link Renderer presets implemented
- ‚úÖ All changes tested and working

**Testing:**
- ‚úÖ All 5 test workflows passing
- ‚úÖ Video workflows working with VideoHelperSuite
- ‚úÖ Optical flow motion detection functional
- ‚úÖ All 8 presets tested visually
- ‚úÖ User testing completed with feedback

**Documentation:**
- ‚úÖ README updated with new nodes
- ‚úÖ Video workflow examples added
- ‚úÖ Preset guide created
- ‚úÖ CHANGELOG updated

**User Experience:**
- ‚úÖ No manual size entry needed
- ‚úÖ Video workflows simplified
- ‚úÖ Presets help users explore options
- ‚úÖ Color distribution automated

---

## üéØ User Testing Checkpoint

### After Phase 1.5 Implementation

**Goal:** Validate usability improvements before expanding to Phase 2

**Testing Focus:**
1. **Ease of Use:** Can users create workflows without errors?
2. **Video Workflows:** Do video workflows work intuitively?
3. **Preset Discovery:** Do presets help users find good settings?
4. **Color Harmony:** Does palette distribution create good results?

**Test Scenarios:**

**Scenario 1: New User - First Workflow**
- Load an image
- Track features
- Render lines with preset
- Success = working result in <5 minutes

**Scenario 2: Video User - Motion Tracking**
- Load a video
- Track motion (optical flow)
- Render motion trails
- Success = visible motion without manual setup

**Scenario 3: Creative User - Style Exploration**
- Try all 8 presets
- Find preferred style
- Customize from preset
- Success = satisfying visual in <10 minutes

**Feedback Collection:**
- Which features were confusing?
- Which presets were most useful?
- Any missing preset styles?
- Video workflow clarity?
- Color distribution satisfaction?

**Go/No-Go Decision:**
- ‚úÖ GO to Phase 2 if: 80%+ test scenarios successful
- üîÑ ITERATE if: Major usability issues found
- ‚ö†Ô∏è HOLD if: Critical bugs discovered

---

## üìù Documentation Updates Needed

### README.md
- Add Image Size Detector section
- Add Video Frame Offset section
- Add video workflow examples
- Document preset options
- Update node count (6 ‚Üí 8 nodes)

### QUICK_START.md
- Update example workflow with size detector
- Add video quickstart section

### TROUBLESHOOTING.md
- Remove manual size entry issues (now auto-detected)
- Add video-specific troubleshooting
- Add preset usage tips

### CHANGELOG.md
- Document v0.1.5 with new features
- List all preset configurations

---

## üöÄ Phase 1.5 Timeline

**Total Estimated Time:** 2-3 weeks

- **Week 1:** Core nodes (Size Detector + Video Offset)
- **Week 2:** Enhancements (Glow default, Palette distribution, Presets)
- **Week 3:** Testing, user validation, documentation

**Critical Path:**
1. Image Size Detector (blocks workflow simplification)
2. Video Frame Offset (blocks video testing)
3. User testing (blocks Phase 2 go-ahead)

---

## üéâ Phase 1.5 Benefits

### For Users

**Workflow Simplicity:**
- ‚úÖ No manual size configuration
- ‚úÖ Video workflows just work
- ‚úÖ Quick exploration with presets
- ‚úÖ Harmonious colors automatically

**Better Defaults:**
- ‚úÖ Glow effects opt-in, not default
- ‚úÖ Clean starting point
- ‚úÖ Gradual complexity increase

**Video Support:**
- ‚úÖ Motion detection works properly
- ‚úÖ Optical flow enabled
- ‚úÖ Temporal effects possible

### For Development

**Foundation for Phase 2:**
- ‚úÖ Size detection pattern established
- ‚úÖ Video workflow validated
- ‚úÖ User feedback incorporated
- ‚úÖ More accurate Phase 2 priorities

**Quality Assurance:**
- ‚úÖ User testing before expansion
- ‚úÖ Validate architecture decisions
- ‚úÖ Catch usability issues early

---

**Phase 1.5 Status:** Planning Complete
**Ready to Implement:** After Phase 1.5 approval
**Next Milestone:** User testing checkpoint before Phase 2
